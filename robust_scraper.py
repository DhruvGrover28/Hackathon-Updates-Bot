#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Robust scraper - Tries Selenium first, falls back to requests gracefully
Fixed version with proper error handling and structure
"""

import requests
from bs4 import BeautifulSoup
import logging
from database import Database
import os
from dotenv import load_dotenv
import time

# Try to import Selenium components
try:
    from selenium.webdriver.common.by import By
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    By = None

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

class RobustHackathonScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        self.driver = None
        self.selenium_available = False
    
    def try_selenium_setup(self):
        """Try to setup Selenium, return True if successful"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from webdriver_manager.chrome import ChromeDriverManager
            from selenium.webdriver.chrome.service import Service
            
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-plugins")
            chrome_options.add_argument("--disable-images")
            chrome_options.add_argument("--log-level=3")
            chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(30)
            self.driver.implicitly_wait(10)
            
            print("‚úÖ Selenium setup successful - using full scraping")
            self.selenium_available = True
            return True
            
        except Exception as e:
            print(f"‚ùå Selenium setup failed: {e}")
            print("üì¶ Falling back to requests-only mode")
            self.selenium_available = False
            return False
    
    def scrape_with_selenium(self):
        """Try selenium-based scraping directly without subprocess"""
        try:
            print("üöÄ Running Selenium-based scraping...")
            
            if not self.selenium_available:
                print("‚ùå Selenium not available, skipping...")
                return False
            
            hackathons = []
            start_time = time.time()
            timeout = 300  # 5 minutes max
            
            # Scrape Unstop with Selenium
            try:
                print("üîç Scraping Unstop with Selenium...")
                self.driver.get("https://unstop.com/hackathons")
                time.sleep(2)  # Reduced wait time
                
                # Wait for content to load and try multiple selectors
                hackathon_selectors = [
                    "div[class*='card']",
                    "div[class*='competition']", 
                    "div[class*='opportunity']",
                    "article",
                    "a[href*='/hackathons/']",
                    "div[class*='list-item']"
                ]
                
                cards = []
                for selector in hackathon_selectors:
                    try:
                        found = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        if found:
                            cards.extend(found)
                            print(f"Found {len(found)} elements with selector: {selector}")
                    except:
                        continue
                
                # Remove duplicates
                cards = list(set(cards))
                print(f"Total unique elements found: {len(cards)}")
                
                for card in cards[:15]:  # Check more cards
                    try:
                        # Try multiple ways to get title text
                        title = ""
                        title_selectors = ["h1", "h2", "h3", "h4", ".title", "[class*='title']", "a"]
                        
                        for t_sel in title_selectors:
                            try:
                                title_elem = card.find_element(By.CSS_SELECTOR, t_sel)
                                title = title_elem.text.strip()
                                if len(title) > 5:
                                    break
                            except:
                                continue
                        
                        if not title:
                            title = card.text.strip()
                        
                        if len(title) < 5 or len(title) > 200:
                            continue
                        
                        # Better filtering for real hackathons
                        skip_terms = ['join', 'participate', 'explore', 'browse', 'filter', 'sort', 'login', 'signup', 'menu', 'nav']
                        if any(skip in title.lower() for skip in skip_terms):
                            continue
                        
                        # Look for hackathon-related keywords
                        hackathon_keywords = ['hackathon', 'hack', 'coding', 'tech', 'innovation', 'challenge', 'competition', 'contest']
                        if not any(keyword in title.lower() for keyword in hackathon_keywords):
                            continue
                        
                        # Try to get URL
                        url = ""
                        try:
                            link_elem = card.find_element(By.CSS_SELECTOR, "a")
                            url = link_elem.get_attribute('href')
                        except:
                            try:
                                url = card.get_attribute('href')
                            except:
                                pass
                        
                        # Validate URL
                        if url and ('/hackathons/' in url or '/competitions/' in url):
                            if not url.startswith('http'):
                                url = f"https://unstop.com{url}"
                                
                            hackathons.append({
                                'title': title,
                                'url': url,
                                'source': 'Unstop',
                                'date_info': 'Check Unstop for dates',
                                'description': f'Hackathon from Unstop: {title}'
                            })
                            print(f"‚úÖ Found Unstop hackathon: {title}")
                            
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"Error scraping Unstop with Selenium: {e}")
            
            # Scrape DevPost with Selenium  
            try:
                print("üîç Scraping DevPost with Selenium...")
                self.driver.get("https://devpost.com/hackathons")
                time.sleep(2)  # Reduced wait time
                
                # DevPost specific selectors
                devpost_selectors = [
                    ".hackathon-tile",
                    "div[class*='hackathon']",
                    "div[class*='challenge']", 
                    "div[class*='tile']"
                ]
                
                cards = []
                for selector in devpost_selectors:
                    try:
                        found = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        if found:
                            cards.extend(found)
                            print(f"Found {len(found)} DevPost elements with selector: {selector}")
                    except:
                        continue
                
                cards = list(set(cards))
                print(f"Total unique DevPost elements: {len(cards)}")
                
                processed = 0
                for card in cards[:5]:  # Reduced from 10 to 5
                    # Check timeout
                    if time.time() - start_time > 30:  # Reduced timeout to 30 seconds
                        print("‚è∞ Timeout reached, stopping DevPost scraping...")
                        break
                        
                    processed += 1
                    try:
                        title = ""
                        
                        # Try h3 first (DevPost uses h3 for hackathon titles)
                        try:
                            title_elem = card.find_element(By.CSS_SELECTOR, "h3")
                            title = title_elem.text.strip()
                            print(f"  Found h3 title: {title}")
                        except:
                            # Fallback to other selectors
                            title_selectors = ["h2", "h1", "h4", ".title", "[class*='title']"]
                            for t_sel in title_selectors:
                                try:
                                    title_elem = card.find_element(By.CSS_SELECTOR, t_sel)
                                    title = title_elem.text.strip()
                                    if len(title) > 5:
                                        print(f"  Found title with {t_sel}: {title}")
                                        break
                                except:
                                    continue
                        
                        if not title:
                            try:
                                # Get all text and try to extract the title
                                full_text = card.text.strip()
                                lines = [line.strip() for line in full_text.split('\n') if line.strip()]
                                # Usually the hackathon name is one of the longer lines
                                for line in lines:
                                    if len(line) > 10 and len(line) < 100:
                                        if not any(skip in line.lower() for skip in ['online', 'days left', 'prizes', 'participants']):
                                            title = line
                                            print(f"  Extracted title from text: {title}")
                                            break
                            except:
                                print(f"  No text found in card {processed}")
                                continue
                        
                        if len(title) < 5 or len(title) > 150:
                            print(f"  Skipping - title too short/long: {len(title)} chars")
                            continue
                            
                        # Enhanced DevPost filtering - be less aggressive since we found real hackathons
                        skip_terms = [
                            'join a hackathon', 'participate in our', 'devpost', 'access your', 
                            'for teams', 'browse', 'explore', 'see all', 'view more'
                        ]
                        if any(term in title.lower() for term in skip_terms):
                            print(f"  Skipping navigation item: {title[:30]}...")
                            continue
                        
                        # Accept most titles that look like hackathon names
                        # Real DevPost hackathons usually have descriptive names
                        if title.lower() in ['hackathons', 'challenges', 'events']:
                            print(f"  Skipping generic term: {title}")
                            continue
                        
                        # Get URL - DevPost uses specific link structure
                        url = ""
                        try:
                            link_elem = card.find_element(By.CSS_SELECTOR, "a")
                            url = link_elem.get_attribute('href')
                            print(f"  Found URL: {url}")
                        except:
                            try:
                                url = card.get_attribute('href')
                            except:
                                print(f"  No URL found for: {title}")
                                continue
                        
                        # DevPost URLs should contain devpost.com
                        if url and 'devpost.com' in url:
                            hackathons.append({
                                'title': title,
                                'url': url,
                                'source': 'DevPost',
                                'date_info': 'Check DevPost for dates', 
                                'description': f'Hackathon from DevPost: {title}'
                            })
                            print(f"‚úÖ Found DevPost hackathon: {title}")
                        else:
                            print(f"  Invalid URL for {title}: {url}")
                            
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"Error scraping DevPost with Selenium: {e}")
            
            # Scrape MLH with Selenium
            try:
                print("üîç Scraping MLH with Selenium...")
                self.driver.get("https://mlh.io/events")
                time.sleep(2)  # Reduced wait time
                
                # MLH specific selectors
                mlh_selectors = [
                    "div[class*='event']",
                    "div[class*='hackathon']",
                    "article",
                    ".event-card",
                    "a[href*='/events/']"
                ]
                
                cards = []
                for selector in mlh_selectors:
                    try:
                        found = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        if found:
                            cards.extend(found)
                            print(f"Found {len(found)} MLH elements with selector: {selector}")
                    except:
                        continue
                
                cards = list(set(cards))
                print(f"Total unique MLH elements: {len(cards)}")
                
                for card in cards[:10]:
                    try:
                        title = ""
                        title_selectors = ["h1", "h2", "h3", "h4", ".title", "[class*='title']", "a"]
                        
                        for t_sel in title_selectors:
                            try:
                                title_elem = card.find_element(By.CSS_SELECTOR, t_sel)
                                title = title_elem.text.strip()
                                if len(title) > 8:
                                    break
                            except:
                                continue
                        
                        if not title:
                            title = card.text.strip()
                        
                        if len(title) < 8 or len(title) > 120:
                            continue
                            
                        # MLH filtering - should be real event names
                        skip_terms = ['events', 'hackathons', 'mlh', 'major league hacking', 'browse', 'see all']
                        if any(term in title.lower() for term in skip_terms):
                            continue
                        
                        # Must have hackathon indicators
                        if not any(word in title.lower() for word in ['hack', 'thon', '2024', '2025', 'hacks']):
                            continue
                        
                        # Get URL
                        url = ""
                        try:
                            link_elem = card.find_element(By.CSS_SELECTOR, "a")
                            url = link_elem.get_attribute('href')
                        except:
                            try:
                                url = card.get_attribute('href')
                            except:
                                pass
                        
                        if url:
                            if not url.startswith('http'):
                                url = f"https://mlh.io{url}"
                                
                            hackathons.append({
                                'title': title,
                                'url': url,
                                'source': 'MLH',
                                'date_info': 'Check MLH for dates',
                                'description': f'Hackathon from MLH: {title}'
                            })
                            print(f"‚úÖ Found MLH hackathon: {title}")
                            
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"Error scraping MLH with Selenium: {e}")
            
            # Add hackathons to database
            if hackathons:
                db = Database()
                added_count = 0
                for hackathon in hackathons:
                    if db.add_hackathon(hackathon['title'], hackathon['url'], hackathon['date_info'], hackathon['description']):
                        added_count += 1
                
                print(f"‚úÖ Added {added_count} new hackathons via Selenium")
                return True
            else:
                print("‚ùå No hackathons found with Selenium")
                return False
                
        except Exception as e:
            print(f"‚ùå Error with Selenium scraping: {e}")
            return False
    
    def scrape_with_requests_fallback(self):
        """Fallback requests-only scraping - Get real hackathon data"""
        try:
            print("üîÑ Using requests-only fallback mode...")
            
            hackathons = []
            
            # Method 1: Scrape Unstop.com directly with better targeting
            try:
                print("üîç Scraping Unstop.com...")
                response = self.session.get("https://unstop.com/hackathons", timeout=20)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # More specific selectors for Unstop
                    # Try different approaches to find hackathon cards
                    
                    # Approach 1: Look for competition cards
                    cards = soup.find_all(['div'], class_=lambda x: x and any(
                        keyword in x.lower() for keyword in ['competition-card', 'event-card', 'opportunity-card']
                    ))
                    
                    # Approach 2: Look for article tags
                    if not cards:
                        cards = soup.find_all(['article', 'div'], class_=lambda x: x and any(
                            keyword in x.lower() for keyword in ['card', 'item', 'list-item']
                        ))
                    
                    # Approach 3: Look for divs containing hackathon links
                    if not cards:
                        all_links = soup.find_all('a', href=lambda x: x and '/hackathons/' in x)
                        cards = [link.find_parent(['div', 'article']) for link in all_links if link.find_parent(['div', 'article'])]
                        cards = [card for card in cards if card]  # Remove None values
                    
                    print(f"Found {len(cards)} potential Unstop hackathon elements")
                    
                    for card in cards[:8]:  # Limit to first 8
                        try:
                            # Extract title - be more specific
                            title_elem = card.find(['h1', 'h2', 'h3', 'h4'], 
                                                  class_=lambda x: x and any(
                                                      keyword in x.lower() for keyword in ['title', 'name', 'heading']
                                                  ))
                            
                            if not title_elem:
                                # Look for any heading in the card
                                title_elem = card.find(['h1', 'h2', 'h3', 'h4', 'h5'])
                            
                            if not title_elem:
                                # Look for the main link text
                                link_elem = card.find('a', href=lambda x: x and '/hackathons/' in x)
                                if link_elem:
                                    title_elem = link_elem
                            
                            if not title_elem:
                                continue
                                
                            title = title_elem.get_text(strip=True)
                            
                            # Filter out navigation and generic text
                            if not title or len(title) < 10:
                                continue
                            
                            # Skip generic/navigation items
                            skip_terms = ['view all', 'see more', 'browse', 'filter', 'sort', 'search', 'menu', 'nav']
                            if any(term in title.lower() for term in skip_terms):
                                continue
                            
                            # Must contain hackathon-related terms
                            if not any(keyword in title.lower() for keyword in ['hackathon', 'hack', 'coding', 'development', 'tech', 'innovation', 'challenge']):
                                continue
                            
                            # Extract link
                            link_elem = card.find('a', href=True)
                            link = ""
                            if link_elem and '/hackathons/' in link_elem['href']:
                                href = link_elem['href']
                                if href.startswith('/'):
                                    link = f"https://unstop.com{href}"
                                elif href.startswith('http'):
                                    link = href
                            
                            # Look for date/deadline info
                            date_text = "Check Unstop for dates"
                            
                            # Look for deadline in various formats
                            date_keywords = ['deadline', 'last date', 'submit by', 'ends on', 'closes', 'due']
                            for keyword in date_keywords:
                                date_elem = card.find(string=lambda text: text and keyword in text.lower())
                                if date_elem:
                                    # Try to extract the date part
                                    import re
                                    date_match = re.search(r'(\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\d{1,2}\s+\w+\s+\d{2,4})', date_elem.parent.get_text())
                                    if date_match:
                                        date_text = date_match.group(1)
                                        break
                            
                            # Look for organization/location
                            org_elem = card.find(['p', 'span', 'div'], class_=lambda x: x and any(
                                keyword in x.lower() for keyword in ['org', 'location', 'institute', 'company', 'by']
                            ))
                            organization = ""
                            if org_elem:
                                org_text = org_elem.get_text(strip=True)
                                if len(org_text) < 100:  # Avoid long descriptions
                                    organization = org_text
                            
                            # Only add if we have a proper link
                            if link:
                                hackathon = {
                                    'title': title,
                                    'link': link,
                                    'deadline': date_text,
                                    'source': 'Unstop',
                                    'organization': organization
                                }
                                hackathons.append(hackathon)
                                print(f"üìù Found: {title}")
                                
                        except Exception as e:
                            print(f"Error processing Unstop card: {e}")
                            continue
                            
            except Exception as e:
                print(f"‚ùå Unstop scraping failed: {e}")
            
            # Method 2: Scrape DevPost
            try:
                print("üîç Scraping DevPost.com...")
                response = self.session.get("https://devpost.com/hackathons", timeout=20)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # DevPost specific selectors - try multiple approaches
                    cards = soup.find_all(['div'], class_=lambda x: x and 'hackathon-tile' in x.lower())
                    
                    if not cards:
                        cards = soup.find_all(['div', 'article'], class_=lambda x: x and any(
                            keyword in x.lower() for keyword in ['hackathon', 'challenge', 'event']
                        ))
                    
                    if not cards:
                        # Look for any cards with hackathon-related content
                        all_divs = soup.find_all('div')
                        cards = [div for div in all_divs if div.find(string=lambda t: t and any(
                            word in t.lower() for word in ['hackathon', 'hack ', 'coding challenge']
                        ))]
                    
                    print(f"Found {len(cards)} potential DevPost events")
                    
                    for card in cards[:8]:  # Increase limit to 8
                        try:
                            # More aggressive title searching
                            title_elem = card.find(['h1', 'h2', 'h3', 'h4', 'a']) 
                            if not title_elem:
                                # Look for any text that looks like a title
                                title_elem = card.find(string=lambda t: t and len(t.strip()) > 15 and any(
                                    word in t.lower() for word in ['hackathon', 'challenge', 'hack']
                                ))
                                if title_elem:
                                    title = title_elem.strip()
                                else:
                                    continue
                            else:
                                title = title_elem.get_text(strip=True)
                            
                            if not title or len(title) < 8:
                                continue
                            
                            # Filter out navigation and generic items for DevPost
                            skip_terms = [
                                'join a hackathon', 'participate in', 'devpost', 'access your', 
                                'for teams', 'hackathons', 'challenges', 'events', 'browse',
                                'see all', 'view more', 'explore', 'find hackathons', 
                                'public hackathons', 'company\'s private', 'register now'
                            ]
                            if any(term in title.lower() for term in skip_terms):
                                continue
                            
                            # Must look like a real hackathon name with specific words/numbers
                            # Real hackathons usually have: numbers, specific names, tech terms
                            valid_indicators = [
                                any(char.isdigit() for char in title),  # Has numbers (version, year)
                                any(word in title.lower() for word in ['2024', '2025', 'ai', 'ml', 'web3', 'crypto', 'tech', 'data', 'climate', 'health', 'fintech']),
                                len(title.split()) >= 2 and len(title) > 20  # Multi-word, descriptive title
                            ]
                            
                            if not any(valid_indicators):
                                continue
                            
                            # Extract link
                            link_elem = card.find('a', href=True)
                            link = ""
                            if link_elem:
                                href = link_elem['href']
                                if href.startswith('/'):
                                    link = f"https://devpost.com{href}"
                                elif href.startswith('http'):
                                    link = href
                            
                            # Look for date info more aggressively
                            date_text = "Check DevPost for dates"
                            
                            # Try to find date in various formats
                            date_patterns = [
                                r'deadline[:\s]+([^<\n]+)',
                                r'due[:\s]+([^<\n]+)',
                                r'ends[:\s]+([^<\n]+)',
                                r'submit by[:\s]+([^<\n]+)'
                            ]
                            
                            import re
                            card_text = card.get_text()
                            for pattern in date_patterns:
                                match = re.search(pattern, card_text, re.IGNORECASE)
                                if match:
                                    date_text = match.group(1).strip()
                                    break
                            
                            # Look for sponsor/organization
                            org_elem = card.find(['span', 'div'], class_=lambda x: x and any(
                                keyword in x.lower() for keyword in ['sponsor', 'host', 'by']
                            ))
                            organization = "DevPost Community"
                            if org_elem:
                                organization = org_elem.get_text(strip=True)
                            
                            # Only add if it looks like a real hackathon
                            if any(keyword in title.lower() for keyword in ['hackathon', 'hack ', 'challenge', 'coding']):
                                hackathon = {
                                    'title': title,
                                    'link': link or "https://devpost.com/hackathons",
                                    'deadline': date_text,
                                    'source': 'DevPost',
                                    'organization': organization
                                }
                                hackathons.append(hackathon)
                                print(f"üìù Found: {title}")
                            
                        except Exception as e:
                            print(f"Error processing DevPost card: {e}")
                            continue
                            
            except Exception as e:
                print(f"‚ùå DevPost scraping failed: {e}")
            
            # Method 3: Try MLH (Major League Hacking)
            try:
                print("üîç Scraping MLH...")
                response = self.session.get("https://mlh.io/events", timeout=20)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Look for event cards
                    events = soup.find_all(['div', 'article'], class_=lambda x: x and any(
                        keyword in x.lower() for keyword in ['event', 'hackathon']
                    ))
                    
                    print(f"Found {len(events)} potential MLH events")
                    
                    for event in events[:5]:
                        try:
                            title_elem = event.find(['h1', 'h2', 'h3', 'h4', 'a'])
                            if not title_elem:
                                continue
                                
                            title = title_elem.get_text(strip=True)
                            
                            if not title or len(title) < 8:
                                continue
                            
                            # Extract link
                            link_elem = event.find('a', href=True)
                            link = ""
                            if link_elem:
                                href = link_elem['href']
                                if href.startswith('/'):
                                    link = f"https://mlh.io{href}"
                                elif href.startswith('http'):
                                    link = href
                            
                            # Look for date
                            date_text = "Check MLH for dates"
                            date_elem = event.find(string=lambda text: text and any(
                                keyword in text.lower() for keyword in ['2024', '2025', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb']
                            ))
                            if date_elem:
                                date_text = date_elem.strip()
                            
                            if 'hack' in title.lower():
                                hackathon = {
                                    'title': title,
                                    'link': link or "https://mlh.io/events",
                                    'deadline': date_text,
                                    'source': 'MLH',
                                    'organization': 'Major League Hacking'
                                }
                                hackathons.append(hackathon)
                                print(f"üìù Found: {title}")
                                
                        except Exception as e:
                            print(f"Error processing MLH event: {e}")
                            continue
                            
            except Exception as e:
                print(f"‚ùå MLH scraping failed: {e}")
            
            # Return only real hackathons found
            if len(hackathons) > 0:
                print(f"‚úÖ Found {len(hackathons)} real hackathons!")
            else:
                print("‚ùå No real hackathons found in this scraping session")
            
            return hackathons
            
        except Exception as e:
            print(f"‚ùå Fallback scraping failed: {e}")
            return []
    
    def send_telegram_notifications(self, hackathons):
        """Send notifications to Telegram"""
        try:
            load_dotenv()
            bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
            channel_id = os.getenv('TELEGRAM_CHANNEL_ID')
            
            if not bot_token or not channel_id:
                print("‚ùå Telegram credentials not found")
                return
            
            print(f"üì§ Sending {len(hackathons)} notifications to {channel_id}")
            
            for hackathon in hackathons:
                # Format message like your example (without emojis in title)
                organization_line = ""
                if hackathon.get('organization') and hackathon['organization']:
                    organization_line = f"{hackathon['organization']}\n"
                
                message = f"""üöÄ {hackathon['title']}
{organization_line}
üìÖ Date: {hackathon['deadline']}

üìù Live from {hackathon['source']}.com (Requests fallback)

üîó {hackathon['link']}

#Hackathon #Competition #Tech #Coding"""
                
                # Send via Telegram API directly
                telegram_url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
                data = {
                    'chat_id': channel_id,
                    'text': message,
                    'parse_mode': 'Markdown',
                    'disable_web_page_preview': True
                }
                
                try:
                    response = requests.post(telegram_url, data=data, timeout=10)
                    if response.status_code == 200:
                        print(f"‚úÖ Sent notification for: {hackathon['title']}")
                    else:
                        print(f"‚ùå Failed to send notification: {response.text}")
                except Exception as e:
                    print(f"‚ùå Error sending to Telegram: {e}")
                
                time.sleep(1)  # Rate limiting
                
        except Exception as e:
            print(f"‚ùå Error in Telegram notifications: {e}")
    
    def run_robust_scraping(self):
        """Run scraping with Selenium first, fallback to requests"""
        try:
            # Try Selenium first
            selenium_success = False
            if self.try_selenium_setup():
                selenium_success = self.scrape_with_selenium()
                if self.driver:
                    self.driver.quit()
            
            # If Selenium failed, use fallback
            if not selenium_success:
                hackathons = self.scrape_with_requests_fallback()
                
                if hackathons:
                    # Save to database
                    db = Database()
                    new_count = 0
                    newly_added = []
                    
                    # Process each hackathon
                    for hackathon in hackathons:
                        if db.add_hackathon(
                            title=hackathon['title'],
                            url=hackathon['link'],
                            date_info=hackathon['deadline'],
                            description=f"Source: {hackathon['source']} | Org: {hackathon.get('organization', 'N/A')}"
                        ):
                            new_count += 1
                            newly_added.append(hackathon)
                    
                    print(f"üíæ Added {new_count} new hackathons to database")
                    
                    # Send notifications for new hackathons only
                    if new_count > 0:
                        self.send_telegram_notifications(newly_added)
                    else:
                        print("‚ÑπÔ∏è No new hackathons to notify about")
                else:
                    print("‚ùå No hackathons found in fallback mode")
            else:
                print("‚úÖ Selenium scraping completed successfully")
                    
        except Exception as e:
            print(f"‚ùå Error in robust scraping: {e}")

def main():
    """Main function for robust scraping"""
    print("ü§ñ Starting robust hackathon scraping...")
    scraper = RobustHackathonScraper()
    scraper.run_robust_scraping()
    print("‚úÖ Robust scraping completed!")

if __name__ == "__main__":
    main()
